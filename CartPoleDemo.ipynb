{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.analyticsvidhya.com/blog/2017/01/introduction-to-reinforcement-learning-implementation/\n",
    "\n",
    "#git clone https://github.com/matthiasplappert/keras-rl.git\n",
    "#cd keras-rl\n",
    "#python setup.py install\n",
    "\n",
    "#pip install gym\n",
    "#pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = 'CartPole-v0'\n",
    "\n",
    "# Get the environment and extract the number of actions available in the Cartpole problem\n",
    "env = gym.make(ENV_NAME)\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Box(4,)\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space.n)\n",
    "print(env.observation_space)  #Box(4) = (x,y) of top and (x,y) of bottom positions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                80        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 34        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 114\n",
      "Trainable params: 114\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5000 steps ...\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/keras_rl-0.4.2-py3.7.egg/rl/memory.py:39: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   79/5000: episode: 1, duration: 3.009s, episode steps: 79, steps per second: 26, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.060 [-0.402, 0.722], loss: 0.429106, mean_absolute_error: 0.496745, mean_q: 0.052343\n",
      "  113/5000: episode: 2, duration: 0.517s, episode steps: 34, steps per second: 66, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.151 [-0.159, 0.753], loss: 0.355710, mean_absolute_error: 0.448637, mean_q: 0.190170\n",
      "  163/5000: episode: 3, duration: 0.832s, episode steps: 50, steps per second: 60, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.082 [-0.295, 0.778], loss: 0.315021, mean_absolute_error: 0.467283, mean_q: 0.322757\n",
      "  196/5000: episode: 4, duration: 0.549s, episode steps: 33, steps per second: 60, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.081 [-0.231, 0.809], loss: 0.280673, mean_absolute_error: 0.505295, mean_q: 0.471406\n",
      "  243/5000: episode: 5, duration: 0.783s, episode steps: 47, steps per second: 60, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.038 [-0.541, 0.745], loss: 0.240262, mean_absolute_error: 0.552155, mean_q: 0.638020\n",
      "  277/5000: episode: 6, duration: 0.566s, episode steps: 34, steps per second: 60, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.093 [-0.207, 0.836], loss: 0.198565, mean_absolute_error: 0.616425, mean_q: 0.841349\n",
      "  316/5000: episode: 7, duration: 0.650s, episode steps: 39, steps per second: 60, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.067 [-0.355, 0.899], loss: 0.155731, mean_absolute_error: 0.682669, mean_q: 1.059114\n",
      "  355/5000: episode: 8, duration: 0.649s, episode steps: 39, steps per second: 60, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.069 [-0.314, 0.812], loss: 0.130759, mean_absolute_error: 0.786456, mean_q: 1.314802\n",
      "  377/5000: episode: 9, duration: 0.366s, episode steps: 22, steps per second: 60, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.110 [-0.353, 0.912], loss: 0.104245, mean_absolute_error: 0.864702, mean_q: 1.534769\n",
      "  393/5000: episode: 10, duration: 0.266s, episode steps: 16, steps per second: 60, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.089 [-0.606, 1.016], loss: 0.092254, mean_absolute_error: 0.933405, mean_q: 1.707084\n",
      "  414/5000: episode: 11, duration: 0.349s, episode steps: 21, steps per second: 60, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.080 [-0.629, 1.258], loss: 0.080449, mean_absolute_error: 0.979468, mean_q: 1.861894\n",
      "  434/5000: episode: 12, duration: 0.332s, episode steps: 20, steps per second: 60, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.088 [-0.551, 1.064], loss: 0.102435, mean_absolute_error: 1.096492, mean_q: 2.053904\n",
      "  457/5000: episode: 13, duration: 0.383s, episode steps: 23, steps per second: 60, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.083 [-0.543, 0.988], loss: 0.082370, mean_absolute_error: 1.141100, mean_q: 2.217963\n",
      "  475/5000: episode: 14, duration: 0.299s, episode steps: 18, steps per second: 60, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.094 [-0.584, 1.242], loss: 0.093866, mean_absolute_error: 1.248608, mean_q: 2.414017\n",
      "  489/5000: episode: 15, duration: 0.234s, episode steps: 14, steps per second: 60, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.089 [-0.606, 1.219], loss: 0.122187, mean_absolute_error: 1.332702, mean_q: 2.576036\n",
      "  501/5000: episode: 16, duration: 0.198s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.114 [-0.779, 1.336], loss: 0.104128, mean_absolute_error: 1.366372, mean_q: 2.681935\n",
      "  516/5000: episode: 17, duration: 0.251s, episode steps: 15, steps per second: 60, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.084 [-0.960, 1.472], loss: 0.136934, mean_absolute_error: 1.457923, mean_q: 2.838904\n",
      "  531/5000: episode: 18, duration: 0.251s, episode steps: 15, steps per second: 60, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.089 [-0.800, 1.397], loss: 0.156264, mean_absolute_error: 1.540248, mean_q: 2.973395\n",
      "  546/5000: episode: 19, duration: 0.248s, episode steps: 15, steps per second: 60, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.116 [-0.767, 1.446], loss: 0.210032, mean_absolute_error: 1.620455, mean_q: 3.125021\n",
      "  557/5000: episode: 20, duration: 0.180s, episode steps: 11, steps per second: 61, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.121 [-0.981, 1.740], loss: 0.187760, mean_absolute_error: 1.680201, mean_q: 3.256230\n",
      "  565/5000: episode: 21, duration: 0.134s, episode steps: 8, steps per second: 59, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.169 [-1.546, 2.557], loss: 0.206013, mean_absolute_error: 1.737344, mean_q: 3.373088\n",
      "  574/5000: episode: 22, duration: 0.150s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.145 [-1.773, 2.813], loss: 0.234374, mean_absolute_error: 1.759214, mean_q: 3.461574\n",
      "  582/5000: episode: 23, duration: 0.132s, episode steps: 8, steps per second: 61, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.157 [-1.531, 2.553], loss: 0.195950, mean_absolute_error: 1.781348, mean_q: 3.532415\n",
      "  591/5000: episode: 24, duration: 0.150s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.142 [-1.354, 2.118], loss: 0.339694, mean_absolute_error: 1.898914, mean_q: 3.648464\n",
      "  602/5000: episode: 25, duration: 0.184s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.092 [-1.187, 1.777], loss: 0.422165, mean_absolute_error: 1.940296, mean_q: 3.647870\n",
      "  611/5000: episode: 26, duration: 0.148s, episode steps: 9, steps per second: 61, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.124 [-0.965, 1.627], loss: 0.223826, mean_absolute_error: 1.898653, mean_q: 3.668079\n",
      "  623/5000: episode: 27, duration: 0.200s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.088 [-1.188, 1.918], loss: 0.277120, mean_absolute_error: 2.004827, mean_q: 3.869726\n",
      "  633/5000: episode: 28, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.116 [-1.228, 2.050], loss: 0.320606, mean_absolute_error: 2.010627, mean_q: 3.964512\n",
      "  644/5000: episode: 29, duration: 0.184s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.148 [-1.131, 1.934], loss: 0.522082, mean_absolute_error: 2.179867, mean_q: 4.138683\n",
      "  654/5000: episode: 30, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.125 [-1.129, 1.813], loss: 0.423618, mean_absolute_error: 2.177744, mean_q: 4.106838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  667/5000: episode: 31, duration: 0.216s, episode steps: 13, steps per second: 60, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.091 [-1.016, 1.693], loss: 0.480848, mean_absolute_error: 2.233629, mean_q: 4.208690\n",
      "  678/5000: episode: 32, duration: 0.183s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.095 [-1.030, 1.750], loss: 0.402849, mean_absolute_error: 2.266804, mean_q: 4.336007\n",
      "  688/5000: episode: 33, duration: 0.165s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.132 [-1.348, 2.173], loss: 0.504009, mean_absolute_error: 2.319485, mean_q: 4.431838\n",
      "  698/5000: episode: 34, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.139 [-1.129, 2.012], loss: 0.294427, mean_absolute_error: 2.319868, mean_q: 4.529332\n",
      "  708/5000: episode: 35, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.134 [-1.204, 2.077], loss: 0.636724, mean_absolute_error: 2.468879, mean_q: 4.740454\n",
      "  720/5000: episode: 36, duration: 0.200s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.101 [-1.005, 1.642], loss: 0.545963, mean_absolute_error: 2.461137, mean_q: 4.717982\n",
      "  733/5000: episode: 37, duration: 0.217s, episode steps: 13, steps per second: 60, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.108 [-0.969, 1.693], loss: 0.616409, mean_absolute_error: 2.537519, mean_q: 4.761531\n",
      "  744/5000: episode: 38, duration: 0.182s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.118 [-0.967, 1.767], loss: 0.709260, mean_absolute_error: 2.600899, mean_q: 4.801330\n",
      "  757/5000: episode: 39, duration: 0.216s, episode steps: 13, steps per second: 60, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.109 [-1.182, 1.954], loss: 0.592680, mean_absolute_error: 2.598204, mean_q: 4.885729\n",
      "  766/5000: episode: 40, duration: 0.149s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.122 [-1.188, 1.892], loss: 0.561902, mean_absolute_error: 2.623182, mean_q: 5.040917\n",
      "  775/5000: episode: 41, duration: 0.149s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.155 [-0.983, 1.766], loss: 0.587214, mean_absolute_error: 2.724061, mean_q: 5.204866\n",
      "  785/5000: episode: 42, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.109 [-1.221, 1.812], loss: 0.494245, mean_absolute_error: 2.716821, mean_q: 5.258921\n",
      "  794/5000: episode: 43, duration: 0.149s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.160 [-0.948, 1.776], loss: 0.790516, mean_absolute_error: 2.799905, mean_q: 5.337940\n",
      "  805/5000: episode: 44, duration: 0.183s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.131 [-1.137, 1.889], loss: 0.535259, mean_absolute_error: 2.778472, mean_q: 5.360483\n",
      "  815/5000: episode: 45, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.151 [-1.140, 2.026], loss: 0.821817, mean_absolute_error: 2.861233, mean_q: 5.424691\n",
      "  826/5000: episode: 46, duration: 0.183s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.152 [-1.143, 2.012], loss: 0.894202, mean_absolute_error: 2.929060, mean_q: 5.505414\n",
      "  836/5000: episode: 47, duration: 0.165s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.153 [-1.139, 1.980], loss: 0.612762, mean_absolute_error: 2.891238, mean_q: 5.526071\n",
      "  846/5000: episode: 48, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.113 [-1.212, 1.985], loss: 0.839167, mean_absolute_error: 2.958632, mean_q: 5.617066\n",
      "  856/5000: episode: 49, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.127 [-1.186, 1.983], loss: 0.601043, mean_absolute_error: 2.972941, mean_q: 5.692312\n",
      "  867/5000: episode: 50, duration: 0.182s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.101 [-1.197, 1.872], loss: 0.745774, mean_absolute_error: 3.027503, mean_q: 5.780430\n",
      "  877/5000: episode: 51, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.122 [-1.144, 1.941], loss: 0.550017, mean_absolute_error: 3.024829, mean_q: 5.893991\n",
      "  890/5000: episode: 52, duration: 0.215s, episode steps: 13, steps per second: 60, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.088 [-1.208, 1.849], loss: 0.742992, mean_absolute_error: 3.117114, mean_q: 6.034170\n",
      "  903/5000: episode: 53, duration: 0.216s, episode steps: 13, steps per second: 60, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.103 [-1.218, 1.909], loss: 0.801636, mean_absolute_error: 3.230029, mean_q: 6.055697\n",
      "  914/5000: episode: 54, duration: 0.184s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.116 [-0.990, 1.713], loss: 0.832938, mean_absolute_error: 3.265681, mean_q: 6.117207\n",
      "  924/5000: episode: 55, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.119 [-0.963, 1.645], loss: 0.849204, mean_absolute_error: 3.275195, mean_q: 6.168419\n",
      "  936/5000: episode: 56, duration: 0.199s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.090 [-1.011, 1.621], loss: 0.858757, mean_absolute_error: 3.312662, mean_q: 6.218605\n",
      "  946/5000: episode: 57, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.117 [-0.996, 1.639], loss: 0.650992, mean_absolute_error: 3.318264, mean_q: 6.316546\n",
      "  958/5000: episode: 58, duration: 0.199s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.117 [-0.955, 1.686], loss: 0.941508, mean_absolute_error: 3.377871, mean_q: 6.408471\n",
      "  967/5000: episode: 59, duration: 0.149s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.120 [-1.019, 1.649], loss: 1.007612, mean_absolute_error: 3.416702, mean_q: 6.407339\n",
      "  978/5000: episode: 60, duration: 0.183s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.111 [-1.026, 1.683], loss: 0.877815, mean_absolute_error: 3.428594, mean_q: 6.444073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  988/5000: episode: 61, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.100 [-0.994, 1.596], loss: 0.847301, mean_absolute_error: 3.457591, mean_q: 6.525866\n",
      " 1000/5000: episode: 62, duration: 0.199s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.110 [-1.175, 2.006], loss: 0.914532, mean_absolute_error: 3.495514, mean_q: 6.673672\n",
      " 1012/5000: episode: 63, duration: 0.199s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.115 [-1.325, 2.013], loss: 0.990485, mean_absolute_error: 3.560205, mean_q: 6.722445\n",
      " 1024/5000: episode: 64, duration: 0.200s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.121 [-1.133, 1.877], loss: 1.008922, mean_absolute_error: 3.577854, mean_q: 6.695130\n",
      " 1037/5000: episode: 65, duration: 0.216s, episode steps: 13, steps per second: 60, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.086 [-0.999, 1.680], loss: 0.895940, mean_absolute_error: 3.596855, mean_q: 6.756635\n",
      " 1050/5000: episode: 66, duration: 0.215s, episode steps: 13, steps per second: 61, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.104 [-0.968, 1.709], loss: 1.222441, mean_absolute_error: 3.706140, mean_q: 6.808255\n",
      " 1063/5000: episode: 67, duration: 0.216s, episode steps: 13, steps per second: 60, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.097 [-0.985, 1.584], loss: 0.906758, mean_absolute_error: 3.667124, mean_q: 6.815939\n",
      " 1075/5000: episode: 68, duration: 0.201s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.120 [-0.964, 1.726], loss: 1.086564, mean_absolute_error: 3.744829, mean_q: 6.955483\n",
      " 1088/5000: episode: 69, duration: 0.216s, episode steps: 13, steps per second: 60, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.090 [-0.776, 1.375], loss: 0.830658, mean_absolute_error: 3.755280, mean_q: 7.028697\n",
      " 1103/5000: episode: 70, duration: 0.249s, episode steps: 15, steps per second: 60, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.109 [-0.770, 1.275], loss: 0.845867, mean_absolute_error: 3.784987, mean_q: 7.169063\n",
      " 1115/5000: episode: 71, duration: 0.200s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.113 [-0.835, 1.431], loss: 1.059837, mean_absolute_error: 3.855677, mean_q: 7.173707\n",
      " 1127/5000: episode: 72, duration: 0.199s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.125 [-0.569, 1.245], loss: 0.838423, mean_absolute_error: 3.848419, mean_q: 7.265191\n",
      " 1143/5000: episode: 73, duration: 0.266s, episode steps: 16, steps per second: 60, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.075 [-0.644, 1.240], loss: 1.127411, mean_absolute_error: 3.922066, mean_q: 7.251054\n",
      " 1155/5000: episode: 74, duration: 0.200s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.134 [-0.761, 1.571], loss: 1.528936, mean_absolute_error: 4.022630, mean_q: 7.261878\n",
      " 1168/5000: episode: 75, duration: 0.215s, episode steps: 13, steps per second: 60, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.106 [-0.764, 1.262], loss: 1.102212, mean_absolute_error: 3.974525, mean_q: 7.313739\n",
      " 1182/5000: episode: 76, duration: 0.234s, episode steps: 14, steps per second: 60, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.121 [-0.555, 1.189], loss: 1.404387, mean_absolute_error: 4.024982, mean_q: 7.411635\n",
      " 1197/5000: episode: 77, duration: 0.249s, episode steps: 15, steps per second: 60, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.084 [-0.765, 1.356], loss: 1.171364, mean_absolute_error: 4.055072, mean_q: 7.484286\n",
      " 1208/5000: episode: 78, duration: 0.182s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.124 [-0.740, 1.217], loss: 1.569361, mean_absolute_error: 4.123844, mean_q: 7.472523\n",
      " 1223/5000: episode: 79, duration: 0.250s, episode steps: 15, steps per second: 60, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.100 [-0.748, 1.257], loss: 1.359875, mean_absolute_error: 4.112874, mean_q: 7.479593\n",
      " 1240/5000: episode: 80, duration: 0.283s, episode steps: 17, steps per second: 60, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.096 [-0.561, 1.310], loss: 1.353607, mean_absolute_error: 4.130958, mean_q: 7.542084\n",
      " 1255/5000: episode: 81, duration: 0.248s, episode steps: 15, steps per second: 60, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.109 [-0.589, 1.299], loss: 1.114373, mean_absolute_error: 4.151445, mean_q: 7.670236\n",
      " 1272/5000: episode: 82, duration: 0.283s, episode steps: 17, steps per second: 60, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.084 [-0.554, 1.133], loss: 1.163684, mean_absolute_error: 4.184302, mean_q: 7.725183\n",
      " 1295/5000: episode: 83, duration: 0.382s, episode steps: 23, steps per second: 60, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.078 [-0.435, 1.103], loss: 1.201586, mean_absolute_error: 4.242780, mean_q: 7.864311\n",
      " 1317/5000: episode: 84, duration: 0.366s, episode steps: 22, steps per second: 60, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.090 [-0.358, 0.848], loss: 1.135170, mean_absolute_error: 4.276106, mean_q: 7.959791\n",
      " 1339/5000: episode: 85, duration: 0.365s, episode steps: 22, steps per second: 60, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.102 [-0.407, 0.943], loss: 0.992010, mean_absolute_error: 4.327966, mean_q: 8.118604\n",
      " 1361/5000: episode: 86, duration: 0.368s, episode steps: 22, steps per second: 60, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.092 [-0.408, 1.217], loss: 1.328764, mean_absolute_error: 4.402856, mean_q: 8.138818\n",
      " 1391/5000: episode: 87, duration: 0.500s, episode steps: 30, steps per second: 60, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.063 [-0.366, 1.089], loss: 1.479138, mean_absolute_error: 4.471268, mean_q: 8.170461\n",
      " 1427/5000: episode: 88, duration: 0.599s, episode steps: 36, steps per second: 60, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.066 [-0.424, 0.882], loss: 1.196434, mean_absolute_error: 4.487401, mean_q: 8.328910\n",
      " 1463/5000: episode: 89, duration: 0.599s, episode steps: 36, steps per second: 60, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.070 [-0.618, 1.069], loss: 1.518182, mean_absolute_error: 4.612356, mean_q: 8.442121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1529/5000: episode: 90, duration: 1.100s, episode steps: 66, steps per second: 60, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.068 [-0.350, 0.781], loss: 1.153327, mean_absolute_error: 4.681675, mean_q: 8.767479\n",
      " 1591/5000: episode: 91, duration: 1.033s, episode steps: 62, steps per second: 60, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.109 [-0.839, 0.513], loss: 1.338410, mean_absolute_error: 4.873975, mean_q: 9.127050\n",
      " 1629/5000: episode: 92, duration: 0.633s, episode steps: 38, steps per second: 60, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.113 [-0.838, 0.310], loss: 1.438148, mean_absolute_error: 4.971040, mean_q: 9.299198\n",
      " 1691/5000: episode: 93, duration: 1.032s, episode steps: 62, steps per second: 60, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.130 [-0.752, 0.294], loss: 1.215322, mean_absolute_error: 5.088013, mean_q: 9.636482\n",
      " 1724/5000: episode: 94, duration: 0.550s, episode steps: 33, steps per second: 60, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.105 [-0.790, 0.184], loss: 1.286385, mean_absolute_error: 5.239656, mean_q: 9.925042\n",
      " 1776/5000: episode: 95, duration: 0.866s, episode steps: 52, steps per second: 60, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.151 [-0.347, 0.756], loss: 1.331584, mean_absolute_error: 5.368155, mean_q: 10.164263\n",
      " 1806/5000: episode: 96, duration: 0.498s, episode steps: 30, steps per second: 60, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.064 [-0.840, 0.441], loss: 1.529475, mean_absolute_error: 5.486257, mean_q: 10.390941\n",
      " 1828/5000: episode: 97, duration: 0.367s, episode steps: 22, steps per second: 60, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.107 [-0.936, 0.377], loss: 1.384174, mean_absolute_error: 5.552973, mean_q: 10.613379\n",
      " 1860/5000: episode: 98, duration: 0.532s, episode steps: 32, steps per second: 60, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.049 [-1.042, 0.440], loss: 1.556922, mean_absolute_error: 5.628098, mean_q: 10.701710\n",
      " 1900/5000: episode: 99, duration: 0.666s, episode steps: 40, steps per second: 60, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.059 [-0.948, 0.379], loss: 1.216415, mean_absolute_error: 5.693613, mean_q: 10.943177\n",
      " 1917/5000: episode: 100, duration: 0.283s, episode steps: 17, steps per second: 60, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.118 [-1.041, 0.367], loss: 1.597022, mean_absolute_error: 5.801569, mean_q: 11.112431\n",
      " 1934/5000: episode: 101, duration: 0.282s, episode steps: 17, steps per second: 60, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.108 [-1.186, 0.605], loss: 1.945843, mean_absolute_error: 5.852757, mean_q: 11.131715\n",
      " 1949/5000: episode: 102, duration: 0.250s, episode steps: 15, steps per second: 60, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.112 [-1.070, 0.577], loss: 2.080264, mean_absolute_error: 5.995907, mean_q: 11.248107\n",
      " 1960/5000: episode: 103, duration: 0.184s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.122 [-2.377, 1.386], loss: 1.719213, mean_absolute_error: 5.980092, mean_q: 11.323548\n",
      " 1971/5000: episode: 104, duration: 0.183s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.119 [-2.738, 1.761], loss: 1.313286, mean_absolute_error: 5.971404, mean_q: 11.492738\n",
      " 1982/5000: episode: 105, duration: 0.182s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.146 [-2.921, 1.747], loss: 1.717353, mean_absolute_error: 6.122805, mean_q: 11.842191\n",
      " 1991/5000: episode: 106, duration: 0.150s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.161 [-2.852, 1.755], loss: 1.400674, mean_absolute_error: 6.186624, mean_q: 11.985043\n",
      " 2003/5000: episode: 107, duration: 0.199s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.106 [-2.068, 1.198], loss: 2.349795, mean_absolute_error: 6.202393, mean_q: 11.827537\n",
      " 2015/5000: episode: 108, duration: 0.200s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.095 [-2.049, 1.226], loss: 1.204769, mean_absolute_error: 6.203411, mean_q: 11.944050\n",
      " 2039/5000: episode: 109, duration: 0.399s, episode steps: 24, steps per second: 60, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: -0.032 [-1.083, 0.630], loss: 2.695517, mean_absolute_error: 6.300396, mean_q: 11.902771\n",
      " 2060/5000: episode: 110, duration: 0.349s, episode steps: 21, steps per second: 60, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.091 [-1.052, 0.413], loss: 2.436648, mean_absolute_error: 6.420667, mean_q: 12.061947\n",
      " 2075/5000: episode: 111, duration: 0.249s, episode steps: 15, steps per second: 60, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.100 [-1.054, 0.582], loss: 1.925797, mean_absolute_error: 6.363884, mean_q: 12.111576\n",
      " 2093/5000: episode: 112, duration: 0.300s, episode steps: 18, steps per second: 60, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.096 [-0.869, 0.421], loss: 1.971540, mean_absolute_error: 6.456409, mean_q: 12.222192\n",
      " 2120/5000: episode: 113, duration: 0.449s, episode steps: 27, steps per second: 60, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.061 [-0.979, 0.419], loss: 1.954485, mean_absolute_error: 6.487604, mean_q: 12.349848\n",
      " 2144/5000: episode: 114, duration: 0.399s, episode steps: 24, steps per second: 60, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: -0.078 [-1.217, 0.566], loss: 2.725494, mean_absolute_error: 6.527147, mean_q: 12.329426\n",
      " 2158/5000: episode: 115, duration: 0.234s, episode steps: 14, steps per second: 60, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.857 [0.000, 1.000], mean observation: -0.118 [-3.071, 1.919], loss: 1.969548, mean_absolute_error: 6.602199, mean_q: 12.547191\n",
      " 2168/5000: episode: 116, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.128 [-3.007, 1.925], loss: 4.026080, mean_absolute_error: 6.781955, mean_q: 12.636490\n",
      " 2176/5000: episode: 117, duration: 0.133s, episode steps: 8, steps per second: 60, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.152 [-2.586, 1.577], loss: 2.710178, mean_absolute_error: 6.709112, mean_q: 12.672074\n",
      " 2184/5000: episode: 118, duration: 0.133s, episode steps: 8, steps per second: 60, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.148 [-2.520, 1.541], loss: 3.435954, mean_absolute_error: 6.883298, mean_q: 12.934086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2192/5000: episode: 119, duration: 0.132s, episode steps: 8, steps per second: 61, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.160 [-2.576, 1.578], loss: 4.131147, mean_absolute_error: 6.723415, mean_q: 12.541424\n",
      " 2200/5000: episode: 120, duration: 0.133s, episode steps: 8, steps per second: 60, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.147 [-2.572, 1.612], loss: 1.688442, mean_absolute_error: 6.678586, mean_q: 12.726779\n",
      " 2209/5000: episode: 121, duration: 0.149s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.173 [-2.848, 1.726], loss: 2.062706, mean_absolute_error: 6.766495, mean_q: 12.947590\n",
      " 2219/5000: episode: 122, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.125 [-3.027, 1.941], loss: 3.154390, mean_absolute_error: 6.887782, mean_q: 13.044878\n",
      " 2228/5000: episode: 123, duration: 0.150s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.165 [-2.297, 1.336], loss: 3.239797, mean_absolute_error: 6.923203, mean_q: 13.024105\n",
      " 2239/5000: episode: 124, duration: 0.183s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.142 [-3.306, 2.149], loss: 2.371064, mean_absolute_error: 6.862985, mean_q: 12.998547\n",
      " 2248/5000: episode: 125, duration: 0.150s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.138 [-2.783, 1.756], loss: 2.376096, mean_absolute_error: 6.845624, mean_q: 12.990180\n",
      " 2256/5000: episode: 126, duration: 0.133s, episode steps: 8, steps per second: 60, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.159 [-2.589, 1.596], loss: 2.028006, mean_absolute_error: 6.765902, mean_q: 12.876524\n",
      " 2264/5000: episode: 127, duration: 0.132s, episode steps: 8, steps per second: 61, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.140 [-2.569, 1.602], loss: 2.506621, mean_absolute_error: 6.873153, mean_q: 13.018789\n",
      " 2275/5000: episode: 128, duration: 0.183s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.107 [-2.747, 1.806], loss: 3.311318, mean_absolute_error: 6.939325, mean_q: 13.026965\n",
      " 2285/5000: episode: 129, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.137 [-3.008, 1.935], loss: 3.120756, mean_absolute_error: 6.941904, mean_q: 13.126289\n",
      " 2299/5000: episode: 130, duration: 0.234s, episode steps: 14, steps per second: 60, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.087 [-1.427, 0.770], loss: 3.402773, mean_absolute_error: 7.091553, mean_q: 13.307503\n",
      " 2314/5000: episode: 131, duration: 0.248s, episode steps: 15, steps per second: 60, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.112 [-1.278, 0.584], loss: 3.457573, mean_absolute_error: 7.112572, mean_q: 13.325168\n",
      " 2333/5000: episode: 132, duration: 0.316s, episode steps: 19, steps per second: 60, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.048 [-1.258, 0.799], loss: 2.924039, mean_absolute_error: 7.134775, mean_q: 13.462201\n",
      " 2388/5000: episode: 133, duration: 0.916s, episode steps: 55, steps per second: 60, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.047 [-0.810, 0.232], loss: 3.848342, mean_absolute_error: 7.197455, mean_q: 13.290014\n",
      " 2421/5000: episode: 134, duration: 0.550s, episode steps: 33, steps per second: 60, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.088 [-0.824, 0.418], loss: 3.372009, mean_absolute_error: 7.175789, mean_q: 13.344279\n",
      " 2444/5000: episode: 135, duration: 0.383s, episode steps: 23, steps per second: 60, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.098 [-0.888, 0.205], loss: 2.911561, mean_absolute_error: 7.188830, mean_q: 13.449427\n",
      " 2491/5000: episode: 136, duration: 0.783s, episode steps: 47, steps per second: 60, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.088 [-0.911, 0.543], loss: 2.672998, mean_absolute_error: 7.225950, mean_q: 13.627182\n",
      " 2539/5000: episode: 137, duration: 0.799s, episode steps: 48, steps per second: 60, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.126 [-0.719, 0.208], loss: 3.400361, mean_absolute_error: 7.377699, mean_q: 13.856383\n",
      " 2580/5000: episode: 138, duration: 0.682s, episode steps: 41, steps per second: 60, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.104 [-0.802, 0.414], loss: 3.394557, mean_absolute_error: 7.484625, mean_q: 14.016909\n",
      " 2623/5000: episode: 139, duration: 0.716s, episode steps: 43, steps per second: 60, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.094 [-0.846, 0.540], loss: 3.219805, mean_absolute_error: 7.486780, mean_q: 14.042134\n",
      " 2664/5000: episode: 140, duration: 0.681s, episode steps: 41, steps per second: 60, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.150 [-0.716, 0.395], loss: 2.706470, mean_absolute_error: 7.543486, mean_q: 14.330559\n",
      " 2710/5000: episode: 141, duration: 0.767s, episode steps: 46, steps per second: 60, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.071 [-0.900, 0.202], loss: 2.771652, mean_absolute_error: 7.661827, mean_q: 14.573396\n",
      " 2741/5000: episode: 142, duration: 0.515s, episode steps: 31, steps per second: 60, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.140 [-0.573, 0.191], loss: 2.293647, mean_absolute_error: 7.687814, mean_q: 14.761432\n",
      " 2826/5000: episode: 143, duration: 1.417s, episode steps: 85, steps per second: 60, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: 0.051 [-0.806, 0.618], loss: 3.277227, mean_absolute_error: 7.882309, mean_q: 14.904972\n",
      " 2910/5000: episode: 144, duration: 1.399s, episode steps: 84, steps per second: 60, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.083 [-0.741, 0.311], loss: 3.584547, mean_absolute_error: 8.035401, mean_q: 15.151866\n",
      " 3003/5000: episode: 145, duration: 1.549s, episode steps: 93, steps per second: 60, episode reward: 93.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.147 [-0.270, 0.929], loss: 2.988759, mean_absolute_error: 8.140230, mean_q: 15.495173\n",
      " 3042/5000: episode: 146, duration: 0.649s, episode steps: 39, steps per second: 60, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.148 [-0.731, 0.336], loss: 3.292429, mean_absolute_error: 8.273860, mean_q: 15.795596\n",
      " 3081/5000: episode: 147, duration: 0.648s, episode steps: 39, steps per second: 60, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.128 [-0.662, 0.238], loss: 3.694323, mean_absolute_error: 8.364771, mean_q: 15.884502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3124/5000: episode: 148, duration: 0.717s, episode steps: 43, steps per second: 60, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.047 [-1.010, 0.588], loss: 3.060229, mean_absolute_error: 8.351409, mean_q: 15.931489\n",
      " 3152/5000: episode: 149, duration: 0.466s, episode steps: 28, steps per second: 60, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.098 [-0.745, 0.276], loss: 3.326830, mean_absolute_error: 8.458238, mean_q: 16.168598\n",
      " 3200/5000: episode: 150, duration: 0.799s, episode steps: 48, steps per second: 60, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.139 [-0.775, 0.232], loss: 3.285151, mean_absolute_error: 8.535563, mean_q: 16.286034\n",
      " 3266/5000: episode: 151, duration: 1.099s, episode steps: 66, steps per second: 60, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.087 [-0.723, 0.553], loss: 3.401065, mean_absolute_error: 8.602748, mean_q: 16.450777\n",
      " 3320/5000: episode: 152, duration: 0.900s, episode steps: 54, steps per second: 60, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.102 [-0.779, 0.171], loss: 3.542894, mean_absolute_error: 8.759353, mean_q: 16.768587\n",
      " 3370/5000: episode: 153, duration: 0.833s, episode steps: 50, steps per second: 60, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.460 [0.000, 1.000], mean observation: -0.112 [-0.723, 0.210], loss: 2.970462, mean_absolute_error: 8.865950, mean_q: 17.042662\n",
      " 3417/5000: episode: 154, duration: 0.782s, episode steps: 47, steps per second: 60, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.115 [-0.748, 0.176], loss: 3.511883, mean_absolute_error: 9.003009, mean_q: 17.312977\n",
      " 3467/5000: episode: 155, duration: 0.834s, episode steps: 50, steps per second: 60, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.460 [0.000, 1.000], mean observation: -0.091 [-0.713, 0.217], loss: 3.607148, mean_absolute_error: 9.129886, mean_q: 17.524168\n",
      " 3504/5000: episode: 156, duration: 0.616s, episode steps: 37, steps per second: 60, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.129 [-0.602, 0.190], loss: 3.590668, mean_absolute_error: 9.206626, mean_q: 17.619108\n",
      " 3554/5000: episode: 157, duration: 0.833s, episode steps: 50, steps per second: 60, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.080 [-0.752, 0.560], loss: 4.411442, mean_absolute_error: 9.330173, mean_q: 17.767979\n",
      " 3608/5000: episode: 158, duration: 0.898s, episode steps: 54, steps per second: 60, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.150 [-0.912, 0.236], loss: 4.658643, mean_absolute_error: 9.297136, mean_q: 17.619564\n",
      " 3808/5000: episode: 159, duration: 3.333s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.142 [-0.403, 0.720], loss: 3.920046, mean_absolute_error: 9.575115, mean_q: 18.385427\n",
      " 3882/5000: episode: 160, duration: 1.232s, episode steps: 74, steps per second: 60, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.148 [-1.038, 0.303], loss: 4.400572, mean_absolute_error: 9.864407, mean_q: 18.845980\n",
      " 3937/5000: episode: 161, duration: 0.915s, episode steps: 55, steps per second: 60, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.141 [-0.525, 0.783], loss: 4.164409, mean_absolute_error: 9.905379, mean_q: 18.999214\n",
      " 3994/5000: episode: 162, duration: 0.950s, episode steps: 57, steps per second: 60, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.150 [-0.364, 0.776], loss: 3.401841, mean_absolute_error: 9.970048, mean_q: 19.192017\n",
      " 4064/5000: episode: 163, duration: 1.166s, episode steps: 70, steps per second: 60, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.133 [-0.959, 0.268], loss: 4.304857, mean_absolute_error: 10.059484, mean_q: 19.302912\n",
      " 4206/5000: episode: 164, duration: 2.365s, episode steps: 142, steps per second: 60, episode reward: 142.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.207 [-0.443, 0.968], loss: 4.149557, mean_absolute_error: 10.249936, mean_q: 19.755714\n",
      " 4272/5000: episode: 165, duration: 1.100s, episode steps: 66, steps per second: 60, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.140 [-0.964, 0.254], loss: 4.141105, mean_absolute_error: 10.401641, mean_q: 20.050140\n",
      " 4350/5000: episode: 166, duration: 1.300s, episode steps: 78, steps per second: 60, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.042 [-1.151, 0.731], loss: 3.442747, mean_absolute_error: 10.539797, mean_q: 20.465607\n",
      " 4430/5000: episode: 167, duration: 1.333s, episode steps: 80, steps per second: 60, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.066 [-0.713, 0.339], loss: 4.874506, mean_absolute_error: 10.781189, mean_q: 20.832392\n",
      " 4487/5000: episode: 168, duration: 0.948s, episode steps: 57, steps per second: 60, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.456 [0.000, 1.000], mean observation: -0.116 [-0.865, 0.241], loss: 4.024732, mean_absolute_error: 10.762064, mean_q: 20.735271\n",
      " 4518/5000: episode: 169, duration: 0.517s, episode steps: 31, steps per second: 60, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.110 [-0.964, 0.371], loss: 3.792552, mean_absolute_error: 10.885638, mean_q: 21.162764\n",
      " 4563/5000: episode: 170, duration: 0.749s, episode steps: 45, steps per second: 60, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.092 [-0.726, 0.222], loss: 4.694808, mean_absolute_error: 11.033370, mean_q: 21.287197\n",
      " 4620/5000: episode: 171, duration: 0.950s, episode steps: 57, steps per second: 60, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.456 [0.000, 1.000], mean observation: -0.147 [-0.947, 0.335], loss: 4.028255, mean_absolute_error: 10.999484, mean_q: 21.261530\n",
      " 4697/5000: episode: 172, duration: 1.283s, episode steps: 77, steps per second: 60, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.073 [-0.868, 0.426], loss: 4.747257, mean_absolute_error: 11.146684, mean_q: 21.577255\n",
      " 4740/5000: episode: 173, duration: 0.715s, episode steps: 43, steps per second: 60, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.442 [0.000, 1.000], mean observation: -0.130 [-1.082, 0.243], loss: 5.205763, mean_absolute_error: 11.181403, mean_q: 21.546814\n",
      " 4919/5000: episode: 174, duration: 2.983s, episode steps: 179, steps per second: 60, episode reward: 179.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.018 [-0.958, 0.523], loss: 4.474126, mean_absolute_error: 11.357361, mean_q: 22.037365\n",
      "done, took 84.972 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1319ad3c8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy = EpsGreedyQPolicy()\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10, target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "\n",
    "# Okay, now it's time to learn something! We visualize the training here for show, but this slows down training quite a lot. \n",
    "dqn.fit(env, nb_steps=5000, visualize=True, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e-Greedy epsilon= 0.1\n"
     ]
    }
   ],
   "source": [
    "print(\"e-Greedy epsilon=\",policy.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 25 episodes ...\n",
      "Episode 1: reward: 68.000, steps: 68\n",
      "Episode 2: reward: 66.000, steps: 66\n",
      "Episode 3: reward: 140.000, steps: 140\n",
      "Episode 4: reward: 147.000, steps: 147\n",
      "Episode 5: reward: 60.000, steps: 60\n",
      "Episode 6: reward: 76.000, steps: 76\n",
      "Episode 7: reward: 69.000, steps: 69\n",
      "Episode 8: reward: 58.000, steps: 58\n",
      "Episode 9: reward: 60.000, steps: 60\n",
      "Episode 10: reward: 79.000, steps: 79\n",
      "Episode 11: reward: 98.000, steps: 98\n",
      "Episode 12: reward: 64.000, steps: 64\n",
      "Episode 13: reward: 74.000, steps: 74\n",
      "Episode 14: reward: 114.000, steps: 114\n",
      "Episode 15: reward: 80.000, steps: 80\n",
      "Episode 16: reward: 90.000, steps: 90\n",
      "Episode 17: reward: 71.000, steps: 71\n",
      "Episode 18: reward: 58.000, steps: 58\n",
      "Episode 19: reward: 71.000, steps: 71\n",
      "Episode 20: reward: 67.000, steps: 67\n",
      "Episode 21: reward: 200.000, steps: 200\n",
      "Episode 22: reward: 76.000, steps: 76\n",
      "Episode 23: reward: 82.000, steps: 82\n",
      "Episode 24: reward: 200.000, steps: 200\n",
      "Episode 25: reward: 66.000, steps: 66\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1319400b8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.test(env, nb_episodes=25, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_DQNAgent__policy',\n",
       " '_DQNAgent__test_policy',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_on_test_begin',\n",
       " '_on_test_end',\n",
       " '_on_train_begin',\n",
       " '_on_train_end',\n",
       " 'backward',\n",
       " 'batch_size',\n",
       " 'compile',\n",
       " 'compiled',\n",
       " 'compute_batch_q_values',\n",
       " 'compute_q_values',\n",
       " 'custom_model_objects',\n",
       " 'delta_clip',\n",
       " 'dueling_type',\n",
       " 'enable_double_dqn',\n",
       " 'enable_dueling_network',\n",
       " 'fit',\n",
       " 'forward',\n",
       " 'gamma',\n",
       " 'get_config',\n",
       " 'layers',\n",
       " 'load_weights',\n",
       " 'memory',\n",
       " 'memory_interval',\n",
       " 'metrics_names',\n",
       " 'model',\n",
       " 'nb_actions',\n",
       " 'nb_steps_warmup',\n",
       " 'policy',\n",
       " 'process_state_batch',\n",
       " 'processor',\n",
       " 'recent_action',\n",
       " 'recent_observation',\n",
       " 'reset_states',\n",
       " 'save_weights',\n",
       " 'step',\n",
       " 'target_model',\n",
       " 'target_model_update',\n",
       " 'test',\n",
       " 'test_policy',\n",
       " 'train_interval',\n",
       " 'trainable_model',\n",
       " 'training',\n",
       " 'update_target_model_hard']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(dqn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_add_inbound_node',\n",
       " '_base_init',\n",
       " '_build_input_shape',\n",
       " '_built',\n",
       " '_check_trainable_weights_consistency',\n",
       " '_collected_trainable_weights',\n",
       " '_compute_previous_mask',\n",
       " '_expects_training_arg',\n",
       " '_feed_input_names',\n",
       " '_feed_input_shapes',\n",
       " '_feed_inputs',\n",
       " '_feed_loss_fns',\n",
       " '_feed_output_names',\n",
       " '_feed_output_shapes',\n",
       " '_feed_outputs',\n",
       " '_feed_sample_weight_modes',\n",
       " '_feed_sample_weights',\n",
       " '_feed_targets',\n",
       " '_function_kwargs',\n",
       " '_get_node_attribute_at_index',\n",
       " '_inbound_nodes',\n",
       " '_init_graph_network',\n",
       " '_init_subclassed_network',\n",
       " '_initial_weights',\n",
       " '_input_coordinates',\n",
       " '_input_layers',\n",
       " '_is_compiled',\n",
       " '_is_graph_network',\n",
       " '_layers',\n",
       " '_layers_by_depth',\n",
       " '_losses',\n",
       " '_make_predict_function',\n",
       " '_make_test_function',\n",
       " '_make_train_function',\n",
       " '_network_nodes',\n",
       " '_node_key',\n",
       " '_nodes_by_depth',\n",
       " '_outbound_nodes',\n",
       " '_output_coordinates',\n",
       " '_output_layers',\n",
       " '_output_mask_cache',\n",
       " '_output_shape_cache',\n",
       " '_output_tensor_cache',\n",
       " '_per_input_losses',\n",
       " '_per_input_updates',\n",
       " '_set_inputs',\n",
       " '_standardize_user_data',\n",
       " '_updated_config',\n",
       " '_updates',\n",
       " '_uses_dynamic_learning_phase',\n",
       " '_uses_inputs_arg',\n",
       " 'add',\n",
       " 'add_loss',\n",
       " 'add_update',\n",
       " 'add_weight',\n",
       " 'assert_input_compatibility',\n",
       " 'build',\n",
       " 'built',\n",
       " 'call',\n",
       " 'compile',\n",
       " 'compute_mask',\n",
       " 'compute_output_shape',\n",
       " 'count_params',\n",
       " 'evaluate',\n",
       " 'evaluate_generator',\n",
       " 'fit',\n",
       " 'fit_generator',\n",
       " 'from_config',\n",
       " 'get_config',\n",
       " 'get_input_at',\n",
       " 'get_input_mask_at',\n",
       " 'get_input_shape_at',\n",
       " 'get_layer',\n",
       " 'get_losses_for',\n",
       " 'get_output_at',\n",
       " 'get_output_mask_at',\n",
       " 'get_output_shape_at',\n",
       " 'get_updates_for',\n",
       " 'get_weights',\n",
       " 'input',\n",
       " 'input_mask',\n",
       " 'input_names',\n",
       " 'input_shape',\n",
       " 'input_spec',\n",
       " 'inputs',\n",
       " 'layers',\n",
       " 'load_weights',\n",
       " 'loss',\n",
       " 'loss_functions',\n",
       " 'loss_weights',\n",
       " 'losses',\n",
       " 'metrics',\n",
       " 'metrics_names',\n",
       " 'metrics_tensors',\n",
       " 'metrics_updates',\n",
       " 'model',\n",
       " 'name',\n",
       " 'non_trainable_weights',\n",
       " 'optimizer',\n",
       " 'output',\n",
       " 'output_mask',\n",
       " 'output_names',\n",
       " 'output_shape',\n",
       " 'outputs',\n",
       " 'pop',\n",
       " 'predict',\n",
       " 'predict_classes',\n",
       " 'predict_function',\n",
       " 'predict_generator',\n",
       " 'predict_on_batch',\n",
       " 'predict_proba',\n",
       " 'reset_states',\n",
       " 'run_internal_graph',\n",
       " 'sample_weight_mode',\n",
       " 'sample_weight_modes',\n",
       " 'sample_weights',\n",
       " 'save',\n",
       " 'save_weights',\n",
       " 'set_weights',\n",
       " 'state_updates',\n",
       " 'stateful',\n",
       " 'stateful_metric_functions',\n",
       " 'stateful_metric_names',\n",
       " 'summary',\n",
       " 'supports_masking',\n",
       " 'targets',\n",
       " 'test_function',\n",
       " 'test_on_batch',\n",
       " 'to_json',\n",
       " 'to_yaml',\n",
       " 'total_loss',\n",
       " 'train_function',\n",
       " 'train_on_batch',\n",
       " 'trainable',\n",
       " 'trainable_weights',\n",
       " 'updates',\n",
       " 'uses_learning_phase',\n",
       " 'weighted_metrics',\n",
       " 'weights']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(dqn.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__func__',\n",
       " '__ge__',\n",
       " '__get__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__self__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(dqn.model.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__add__',\n",
       " '__class__',\n",
       " '__contains__',\n",
       " '__delattr__',\n",
       " '__delitem__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__iadd__',\n",
       " '__imul__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__mul__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__reversed__',\n",
       " '__rmul__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " 'append',\n",
       " 'clear',\n",
       " 'copy',\n",
       " 'count',\n",
       " 'extend',\n",
       " 'index',\n",
       " 'insert',\n",
       " 'pop',\n",
       " 'remove',\n",
       " 'reverse',\n",
       " 'sort']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(dqn.model.outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function list.count(value, /)>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.model.outputs.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
